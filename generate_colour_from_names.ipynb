{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "os.chdir(\"../Color-Names\")\n",
    "os.getcwd()\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec downloaded from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing.\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"sw-colors-name-csp-acb.acb\", \"r\")\n",
    "rgbs = {}\n",
    "line = f.readline()\n",
    "while line != \"\":\n",
    "    if \"colorName\" in line:\n",
    "        name = line.split(\">\")[1].split(\"(\")[0].strip()\n",
    "        rgb = {}\n",
    "        line = f.readline()\n",
    "        for color in [\"r\", \"g\", \"b\"]:\n",
    "            line = f.readline()\n",
    "            val = int(line.split(\">\")[1].split(\"<\")[0])\n",
    "            rgb[color] = val\n",
    "\n",
    "        rgbs[name] = rgb\n",
    "\n",
    "    line = f.readline()\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = word2vec.vector_size\n",
    "max_tokens = max([len(name.split()) for name in rgbs])\n",
    "avg_vector = np.mean(word2vec.vectors, axis=0)\n",
    "empty_vector = np.zeros(dim)\n",
    "\n",
    "(X_train, Y_train) = ([], [])\n",
    "\n",
    "\n",
    "for name in rgbs:\n",
    "    \n",
    "    tokens = name.lower().split()\n",
    "    x = [empty_vector] * max_tokens\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in word2vec:\n",
    "            x[i] = word2vec[token]\n",
    "        else:\n",
    "            x[i] = avg_vector\n",
    "\n",
    "    y = [rgbs[name][i] for i in ['r', 'g', 'b']]   \n",
    "    \n",
    "    X_train.append(np.array(x))\n",
    "    Y_train.append(np.array(y))\n",
    "\n",
    "\n",
    "\n",
    "# Add basic colour samples (repeat them to give them higher importance in learning)\n",
    "\n",
    "basic_colours = {\n",
    "    \"Red\": {\"r\": 255, \"g\": 0, \"b\": 0},\n",
    "    \"Blue\": {\"r\": 0, \"g\": 0, \"b\": 255},\n",
    "    \"Green\": {\"r\": 0, \"g\": 255, \"b\": 0},\n",
    "    \"Yellow\": {\"r\": 255, \"g\": 255, \"b\": 0},\n",
    "    \"Magenta\": {\"r\": 255, \"g\": 0, \"b\": 255},\n",
    "    \"Cyan\": {\"r\": 0, \"g\": 255, \"b\": 255},\n",
    "    \"White\": {\"r\": 255, \"g\": 255, \"b\": 255},\n",
    "}\n",
    "importance_weight = 25\n",
    "for i in range(importance_weight):\n",
    "    for colour in basic_colours:\n",
    "        x = [empty_vector] * max_tokens\n",
    "        x[0] = word2vec[colour]\n",
    "        \n",
    "        y = [basic_colours[colour][c] for c in ['r', 'g', 'b']]  \n",
    "        \n",
    "        X_train.append(np.array(x))\n",
    "        Y_train.append(np.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "perm = np.random.permutation(len(X_train))\n",
    "X_train = X_train[perm]\n",
    "Y_train = Y_train[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701, 4, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "48/48 [==============================] - 1s 4ms/step - loss: 5767.3667 - val_loss: 3701.5549\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3701.55493, saving model to model_params.h5\n",
      "Epoch 2/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 3770.1584 - val_loss: 2847.5286\n",
      "\n",
      "Epoch 00002: val_loss improved from 3701.55493 to 2847.52856, saving model to model_params.h5\n",
      "Epoch 3/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 3033.6206 - val_loss: 2738.7449\n",
      "\n",
      "Epoch 00003: val_loss improved from 2847.52856 to 2738.74487, saving model to model_params.h5\n",
      "Epoch 4/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 2742.9507 - val_loss: 2545.1501\n",
      "\n",
      "Epoch 00004: val_loss improved from 2738.74487 to 2545.15015, saving model to model_params.h5\n",
      "Epoch 5/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 2446.3511 - val_loss: 2599.3965\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2545.15015\n",
      "Epoch 6/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 2402.1819 - val_loss: 2472.5657\n",
      "\n",
      "Epoch 00006: val_loss improved from 2545.15015 to 2472.56567, saving model to model_params.h5\n",
      "Epoch 7/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 2280.7686 - val_loss: 2139.5723\n",
      "\n",
      "Epoch 00007: val_loss improved from 2472.56567 to 2139.57227, saving model to model_params.h5\n",
      "Epoch 8/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 2090.3169 - val_loss: 2688.3721\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2139.57227\n",
      "Epoch 9/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 2019.8242 - val_loss: 2027.1375\n",
      "\n",
      "Epoch 00009: val_loss improved from 2139.57227 to 2027.13745, saving model to model_params.h5\n",
      "Epoch 10/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1945.3947 - val_loss: 2090.7939\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2027.13745\n",
      "Epoch 11/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1823.8461 - val_loss: 2138.8948\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2027.13745\n",
      "Epoch 12/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1904.2631 - val_loss: 2247.7842\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2027.13745\n",
      "Epoch 13/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1725.8716 - val_loss: 2156.7156\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2027.13745\n",
      "Epoch 14/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1674.3896 - val_loss: 2098.2742\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2027.13745\n",
      "Epoch 15/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1663.2703 - val_loss: 2135.5811\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2027.13745\n",
      "Epoch 16/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1575.9971 - val_loss: 2094.9392\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2027.13745\n",
      "Epoch 17/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1538.5535 - val_loss: 2218.3013\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2027.13745\n",
      "Epoch 18/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1494.6147 - val_loss: 2159.2773\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2027.13745\n",
      "Epoch 19/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1533.4922 - val_loss: 2150.0896\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2027.13745\n",
      "Epoch 20/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1488.8998 - val_loss: 2251.1995\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2027.13745\n",
      "Epoch 21/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1485.6072 - val_loss: 2162.1050\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2027.13745\n",
      "Epoch 22/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1406.0516 - val_loss: 2235.3818\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2027.13745\n",
      "Epoch 23/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1413.2158 - val_loss: 2093.5410\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2027.13745\n",
      "Epoch 24/500\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 1408.4642 - val_loss: 2102.1021\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2027.13745\n",
      "Epoch 25/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1391.7239 - val_loss: 2242.0369\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2027.13745\n",
      "Epoch 26/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1326.3252 - val_loss: 2293.1926\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2027.13745\n",
      "Epoch 27/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1338.3812 - val_loss: 2116.8604\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2027.13745\n",
      "Epoch 28/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1328.8030 - val_loss: 2229.3877\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2027.13745\n",
      "Epoch 29/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1284.6580 - val_loss: 2156.9163\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2027.13745\n",
      "Epoch 30/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1296.7504 - val_loss: 2256.1489\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2027.13745\n",
      "Epoch 31/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1298.6083 - val_loss: 2209.0217\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2027.13745\n",
      "Epoch 32/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1214.7655 - val_loss: 2143.6660\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2027.13745\n",
      "Epoch 33/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1251.5286 - val_loss: 2195.0527\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2027.13745\n",
      "Epoch 34/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1202.4374 - val_loss: 2128.7866\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2027.13745\n",
      "Epoch 35/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1260.8259 - val_loss: 2117.2629\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2027.13745\n",
      "Epoch 36/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1211.7866 - val_loss: 2095.3799\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2027.13745\n",
      "Epoch 37/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1214.1196 - val_loss: 2116.8230\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2027.13745\n",
      "Epoch 38/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1129.5787 - val_loss: 2332.9146\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2027.13745\n",
      "Epoch 39/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1152.0317 - val_loss: 2226.2043\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2027.13745\n",
      "Epoch 40/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1192.2855 - val_loss: 2291.5413\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2027.13745\n",
      "Epoch 41/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1164.2705 - val_loss: 2155.7998\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2027.13745\n",
      "Epoch 42/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1162.8052 - val_loss: 2158.7109\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2027.13745\n",
      "Epoch 43/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1223.8245 - val_loss: 2121.0327\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2027.13745\n",
      "Epoch 44/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1114.7340 - val_loss: 2378.6990\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2027.13745\n",
      "Epoch 45/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1127.7050 - val_loss: 2207.7495\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2027.13745\n",
      "Epoch 46/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1118.6107 - val_loss: 2158.1538\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2027.13745\n",
      "Epoch 47/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1141.1699 - val_loss: 2177.2019\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2027.13745\n",
      "Epoch 48/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1087.6915 - val_loss: 2277.0718\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2027.13745\n",
      "Epoch 49/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1120.8646 - val_loss: 2136.7988\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2027.13745\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 1115.9070 - val_loss: 2173.9019\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2027.13745\n",
      "Epoch 51/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1090.7593 - val_loss: 2248.5461\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2027.13745\n",
      "Epoch 52/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1127.4755 - val_loss: 2179.9553\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2027.13745\n",
      "Epoch 53/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1067.0439 - val_loss: 2090.9043\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2027.13745\n",
      "Epoch 54/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1092.4802 - val_loss: 2327.1863\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2027.13745\n",
      "Epoch 55/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1044.6398 - val_loss: 2153.2395\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2027.13745\n",
      "Epoch 56/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1104.0389 - val_loss: 2153.3042\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2027.13745\n",
      "Epoch 57/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1049.0110 - val_loss: 2211.1226\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2027.13745\n",
      "Epoch 58/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1075.9297 - val_loss: 2121.2107\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2027.13745\n",
      "Epoch 59/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1049.4386 - val_loss: 2159.9565\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2027.13745\n",
      "Epoch 60/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1033.3011 - val_loss: 2164.4131\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2027.13745\n",
      "Epoch 61/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1066.9053 - val_loss: 2173.5872\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2027.13745\n",
      "Epoch 62/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1017.5731 - val_loss: 2225.0208\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2027.13745\n",
      "Epoch 63/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 993.4979 - val_loss: 2182.2151\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2027.13745\n",
      "Epoch 64/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1024.1683 - val_loss: 2151.3914\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2027.13745\n",
      "Epoch 65/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1021.2900 - val_loss: 2170.7332\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2027.13745\n",
      "Epoch 66/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1021.6491 - val_loss: 2153.3733\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2027.13745\n",
      "Epoch 67/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 964.9190 - val_loss: 2224.2532\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2027.13745\n",
      "Epoch 68/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1042.6029 - val_loss: 2166.8135\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2027.13745\n",
      "Epoch 69/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1048.5841 - val_loss: 2146.5039\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2027.13745\n",
      "Epoch 70/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 989.4833 - val_loss: 2165.8347\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2027.13745\n",
      "Epoch 71/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1016.0253 - val_loss: 2271.4656\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2027.13745\n",
      "Epoch 72/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1035.1823 - val_loss: 2188.4387\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2027.13745\n",
      "Epoch 73/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 984.4614 - val_loss: 2217.4358\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2027.13745\n",
      "Epoch 74/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 956.4198 - val_loss: 2167.3740\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2027.13745\n",
      "Epoch 75/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 983.1789 - val_loss: 2219.3130\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2027.13745\n",
      "Epoch 76/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 957.0980 - val_loss: 2201.4238\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2027.13745\n",
      "Epoch 77/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 953.1321 - val_loss: 2268.1816\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2027.13745\n",
      "Epoch 78/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 968.7205 - val_loss: 2125.3914\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2027.13745\n",
      "Epoch 79/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 924.2105 - val_loss: 2152.3711\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2027.13745\n",
      "Epoch 80/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 985.0374 - val_loss: 2125.3931\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2027.13745\n",
      "Epoch 81/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 959.4537 - val_loss: 2129.2410\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2027.13745\n",
      "Epoch 82/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 968.4966 - val_loss: 2099.5479\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2027.13745\n",
      "Epoch 83/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 972.7065 - val_loss: 2131.7622\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2027.13745\n",
      "Epoch 84/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 962.6431 - val_loss: 2127.6621\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2027.13745\n",
      "Epoch 85/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 947.5025 - val_loss: 2070.1919\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2027.13745\n",
      "Epoch 86/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 970.4501 - val_loss: 2097.7222\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2027.13745\n",
      "Epoch 87/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 954.4500 - val_loss: 2063.1704\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2027.13745\n",
      "Epoch 88/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 944.8275 - val_loss: 2087.5271\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2027.13745\n",
      "Epoch 89/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 924.8555 - val_loss: 2139.9138\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2027.13745\n",
      "Epoch 90/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 915.2323 - val_loss: 2184.2332\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2027.13745\n",
      "Epoch 91/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 910.9635 - val_loss: 2146.5940\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2027.13745\n",
      "Epoch 92/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 962.9821 - val_loss: 2114.3364\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2027.13745\n",
      "Epoch 93/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 926.3653 - val_loss: 2075.0417\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2027.13745\n",
      "Epoch 94/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 902.4570 - val_loss: 2130.4619\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2027.13745\n",
      "Epoch 95/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 898.0396 - val_loss: 2133.4663\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2027.13745\n",
      "Epoch 96/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 926.8499 - val_loss: 2062.2000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2027.13745\n",
      "Epoch 97/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 908.6824 - val_loss: 2049.7886\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2027.13745\n",
      "Epoch 98/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 929.5784 - val_loss: 2063.5923\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2027.13745\n",
      "Epoch 99/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 907.1237 - val_loss: 2123.0898\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2027.13745\n",
      "Epoch 100/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 905.5840 - val_loss: 2100.1060\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2027.13745\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 889.2388 - val_loss: 2097.0610\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2027.13745\n",
      "Epoch 102/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 857.3591 - val_loss: 2126.3801\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 2027.13745\n",
      "Epoch 103/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 884.2900 - val_loss: 2123.9121\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 2027.13745\n",
      "Epoch 104/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 905.7383 - val_loss: 2098.5315\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 2027.13745\n",
      "Epoch 105/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 911.3438 - val_loss: 2106.3862\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 2027.13745\n",
      "Epoch 106/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 903.4473 - val_loss: 2187.9028\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 2027.13745\n",
      "Epoch 107/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 907.4785 - val_loss: 2104.8110\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 2027.13745\n",
      "Epoch 108/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 899.3226 - val_loss: 2127.2634\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 2027.13745\n",
      "Epoch 109/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 873.9266 - val_loss: 2159.7766\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 2027.13745\n",
      "Epoch 110/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 880.5427 - val_loss: 2104.6658\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 2027.13745\n",
      "Epoch 111/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 869.8912 - val_loss: 2132.6240\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 2027.13745\n",
      "Epoch 112/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 871.0881 - val_loss: 2133.2764\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 2027.13745\n",
      "Epoch 113/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 857.7178 - val_loss: 2096.8960\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 2027.13745\n",
      "Epoch 114/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 926.0002 - val_loss: 2081.1987\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 2027.13745\n",
      "Epoch 115/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 858.9983 - val_loss: 2181.7932\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 2027.13745\n",
      "Epoch 116/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 871.8279 - val_loss: 2101.7976\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 2027.13745\n",
      "Epoch 117/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 876.6533 - val_loss: 2265.9446\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 2027.13745\n",
      "Epoch 118/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 891.4655 - val_loss: 2105.5645\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 2027.13745\n",
      "Epoch 119/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 855.1933 - val_loss: 2128.6340\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 2027.13745\n",
      "Epoch 120/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 873.8294 - val_loss: 2087.7710\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 2027.13745\n",
      "Epoch 121/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 859.2695 - val_loss: 2197.2639\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 2027.13745\n",
      "Epoch 122/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 885.2379 - val_loss: 2084.5120\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 2027.13745\n",
      "Epoch 123/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 887.2758 - val_loss: 2073.9126\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 2027.13745\n",
      "Epoch 124/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 851.3294 - val_loss: 2142.3279\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 2027.13745\n",
      "Epoch 125/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 848.5818 - val_loss: 2121.9678\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 2027.13745\n",
      "Epoch 126/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 853.7097 - val_loss: 2080.2954\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 2027.13745\n",
      "Epoch 127/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 863.8851 - val_loss: 2110.0369\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 2027.13745\n",
      "Epoch 128/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 868.4291 - val_loss: 2097.1179\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 2027.13745\n",
      "Epoch 129/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 835.5133 - val_loss: 2170.7156\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 2027.13745\n",
      "Epoch 130/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 855.4264 - val_loss: 2087.0710\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 2027.13745\n",
      "Epoch 131/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 844.8135 - val_loss: 2082.9783\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 2027.13745\n",
      "Epoch 132/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 877.7463 - val_loss: 2125.5745\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 2027.13745\n",
      "Epoch 133/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 823.6630 - val_loss: 2129.6421\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 2027.13745\n",
      "Epoch 134/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 870.0081 - val_loss: 2121.8318\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 2027.13745\n",
      "Epoch 135/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 842.5278 - val_loss: 2142.4089\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 2027.13745\n",
      "Epoch 136/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 866.4006 - val_loss: 2110.7190\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 2027.13745\n",
      "Epoch 137/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 835.9554 - val_loss: 2080.5884\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 2027.13745\n",
      "Epoch 138/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 825.1713 - val_loss: 2119.6184\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 2027.13745\n",
      "Epoch 139/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 790.0008 - val_loss: 2062.3572\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 2027.13745\n",
      "Epoch 140/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 857.8514 - val_loss: 2100.3171\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 2027.13745\n",
      "Epoch 141/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 853.7112 - val_loss: 2132.2341\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 2027.13745\n",
      "Epoch 142/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 860.0670 - val_loss: 2208.0571\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 2027.13745\n",
      "Epoch 143/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 803.1320 - val_loss: 2072.8801\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 2027.13745\n",
      "Epoch 144/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 815.0342 - val_loss: 2088.8752\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 2027.13745\n",
      "Epoch 145/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 821.2172 - val_loss: 2122.3320\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 2027.13745\n",
      "Epoch 146/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 807.3002 - val_loss: 2109.3521\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 2027.13745\n",
      "Epoch 147/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 822.6147 - val_loss: 2161.8584\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 2027.13745\n",
      "Epoch 148/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 792.5393 - val_loss: 2131.1309\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 2027.13745\n",
      "Epoch 149/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 821.6061 - val_loss: 2123.4614\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 2027.13745\n",
      "Epoch 150/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 820.9167 - val_loss: 2094.2981\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 2027.13745\n",
      "Epoch 151/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 784.5960 - val_loss: 2103.8384\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 2027.13745\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 796.2646 - val_loss: 2077.1611\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 2027.13745\n",
      "Epoch 153/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 801.8916 - val_loss: 2147.1167\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 2027.13745\n",
      "Epoch 154/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 820.9251 - val_loss: 2160.8853\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 2027.13745\n",
      "Epoch 155/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 805.7606 - val_loss: 2120.1228\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 2027.13745\n",
      "Epoch 156/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 810.4218 - val_loss: 2110.0408\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 2027.13745\n",
      "Epoch 157/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 803.6052 - val_loss: 2162.5029\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 2027.13745\n",
      "Epoch 158/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 814.9658 - val_loss: 2229.2598\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 2027.13745\n",
      "Epoch 159/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 804.0653 - val_loss: 2191.2493\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 2027.13745\n",
      "Epoch 160/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 843.7552 - val_loss: 2092.1108\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 2027.13745\n",
      "Epoch 161/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 801.6401 - val_loss: 2122.4385\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 2027.13745\n",
      "Epoch 162/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 783.3540 - val_loss: 2171.8276\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 2027.13745\n",
      "Epoch 163/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 806.8943 - val_loss: 2149.7156\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 2027.13745\n",
      "Epoch 164/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 790.4776 - val_loss: 2147.6111\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 2027.13745\n",
      "Epoch 165/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 803.6232 - val_loss: 2129.4851\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 2027.13745\n",
      "Epoch 166/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 823.0485 - val_loss: 2114.1997\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 2027.13745\n",
      "Epoch 167/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 806.2642 - val_loss: 2129.6921\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 2027.13745\n",
      "Epoch 168/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 800.7206 - val_loss: 2096.1228\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 2027.13745\n",
      "Epoch 169/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 794.5986 - val_loss: 2084.1807\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 2027.13745\n",
      "Epoch 170/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 810.2268 - val_loss: 2129.8223\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 2027.13745\n",
      "Epoch 171/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 803.0745 - val_loss: 2100.5186\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 2027.13745\n",
      "Epoch 172/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 781.0323 - val_loss: 2169.3296\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 2027.13745\n",
      "Epoch 173/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 790.1420 - val_loss: 2137.6421\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 2027.13745\n",
      "Epoch 174/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 741.9041 - val_loss: 2132.9248\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 2027.13745\n",
      "Epoch 175/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 754.8627 - val_loss: 2130.8110\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 2027.13745\n",
      "Epoch 176/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 763.3317 - val_loss: 2176.8433\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 2027.13745\n",
      "Epoch 177/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 776.8038 - val_loss: 2142.8906\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 2027.13745\n",
      "Epoch 178/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 758.5938 - val_loss: 2056.0454\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 2027.13745\n",
      "Epoch 179/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 747.9600 - val_loss: 2165.8997\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 2027.13745\n",
      "Epoch 180/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 764.9940 - val_loss: 2098.3896\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 2027.13745\n",
      "Epoch 181/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 787.9376 - val_loss: 2095.5889\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 2027.13745\n",
      "Epoch 182/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 789.8091 - val_loss: 2110.8726\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 2027.13745\n",
      "Epoch 183/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 761.6421 - val_loss: 2098.5750\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 2027.13745\n",
      "Epoch 184/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 757.5333 - val_loss: 2156.8206\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 2027.13745\n",
      "Epoch 185/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 765.3537 - val_loss: 2150.0046\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 2027.13745\n",
      "Epoch 186/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 791.3276 - val_loss: 2126.9912\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 2027.13745\n",
      "Epoch 187/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 761.4023 - val_loss: 2217.3079\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 2027.13745\n",
      "Epoch 188/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 773.2738 - val_loss: 2120.3398\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 2027.13745\n",
      "Epoch 189/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 801.7720 - val_loss: 2085.1885\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 2027.13745\n",
      "Epoch 190/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 771.3601 - val_loss: 2087.8977\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 2027.13745\n",
      "Epoch 191/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 772.0581 - val_loss: 2093.9316\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 2027.13745\n",
      "Epoch 192/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 759.2893 - val_loss: 2077.3606\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 2027.13745\n",
      "Epoch 193/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 766.8740 - val_loss: 2112.3203\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 2027.13745\n",
      "Epoch 194/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 744.0217 - val_loss: 2083.1030\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 2027.13745\n",
      "Epoch 195/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 780.4684 - val_loss: 2175.8059\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 2027.13745\n",
      "Epoch 196/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 742.1970 - val_loss: 2126.9937\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 2027.13745\n",
      "Epoch 197/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 748.8450 - val_loss: 2086.3274\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 2027.13745\n",
      "Epoch 198/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 764.3818 - val_loss: 2073.5647\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 2027.13745\n",
      "Epoch 199/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 769.5635 - val_loss: 2087.2256\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 2027.13745\n",
      "Epoch 200/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 767.6959 - val_loss: 2088.4829\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 2027.13745\n",
      "Epoch 201/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 755.1562 - val_loss: 2067.3721\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 2027.13745\n",
      "Epoch 202/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 775.0129 - val_loss: 2117.6597\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 2027.13745\n",
      "Epoch 203/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 744.4298 - val_loss: 2111.0002\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 2027.13745\n",
      "Epoch 204/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 742.1783 - val_loss: 2179.8149\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 2027.13745\n",
      "Epoch 205/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 742.7092 - val_loss: 2085.7104\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 2027.13745\n",
      "Epoch 206/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 760.8244 - val_loss: 2063.6042\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 2027.13745\n",
      "Epoch 207/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 748.4696 - val_loss: 2095.4270\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 2027.13745\n",
      "Epoch 208/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 744.4840 - val_loss: 2161.4336\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 2027.13745\n",
      "Epoch 209/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 777.9006 - val_loss: 2123.4106\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 2027.13745\n",
      "Epoch 210/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 740.3901 - val_loss: 2152.6453\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 2027.13745\n",
      "Epoch 211/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 762.3173 - val_loss: 2116.4893\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 2027.13745\n",
      "Epoch 212/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 737.8410 - val_loss: 2107.9963\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 2027.13745\n",
      "Epoch 213/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 737.0814 - val_loss: 2148.5771\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 2027.13745\n",
      "Epoch 214/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 742.9880 - val_loss: 2130.6858\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 2027.13745\n",
      "Epoch 215/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 734.7241 - val_loss: 2122.4917\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 2027.13745\n",
      "Epoch 216/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 734.8199 - val_loss: 2062.6868\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 2027.13745\n",
      "Epoch 217/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 735.9621 - val_loss: 2076.3052\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 2027.13745\n",
      "Epoch 218/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 739.2270 - val_loss: 2130.4377\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 2027.13745\n",
      "Epoch 219/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 755.0414 - val_loss: 2117.5452\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 2027.13745\n",
      "Epoch 220/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 743.7028 - val_loss: 2119.4602\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 2027.13745\n",
      "Epoch 221/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 746.7692 - val_loss: 2110.3108\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 2027.13745\n",
      "Epoch 222/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 736.0843 - val_loss: 2105.7148\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 2027.13745\n",
      "Epoch 223/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 741.2050 - val_loss: 2148.3948\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 2027.13745\n",
      "Epoch 224/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 744.7884 - val_loss: 2098.2661\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 2027.13745\n",
      "Epoch 225/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 722.3588 - val_loss: 2100.9314\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 2027.13745\n",
      "Epoch 226/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 745.4294 - val_loss: 2113.0847\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 2027.13745\n",
      "Epoch 227/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 741.1378 - val_loss: 2240.3254\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 2027.13745\n",
      "Epoch 228/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 754.1667 - val_loss: 2109.6633\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 2027.13745\n",
      "Epoch 229/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 739.2898 - val_loss: 2093.9080\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 2027.13745\n",
      "Epoch 230/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 718.1983 - val_loss: 2060.9722\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 2027.13745\n",
      "Epoch 231/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 754.7111 - val_loss: 2063.0903\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 2027.13745\n",
      "Epoch 232/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 742.9394 - val_loss: 2086.8550\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 2027.13745\n",
      "Epoch 233/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 718.0938 - val_loss: 2086.9561\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 2027.13745\n",
      "Epoch 234/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 719.1846 - val_loss: 2110.1851\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 2027.13745\n",
      "Epoch 235/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 712.7067 - val_loss: 2065.5168\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 2027.13745\n",
      "Epoch 236/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 769.5734 - val_loss: 2093.9280\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 2027.13745\n",
      "Epoch 237/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 762.1566 - val_loss: 2049.4153\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 2027.13745\n",
      "Epoch 238/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 705.3604 - val_loss: 2116.6111\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 2027.13745\n",
      "Epoch 239/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 702.7807 - val_loss: 2104.0999\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 2027.13745\n",
      "Epoch 240/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 721.1818 - val_loss: 2077.7671\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 2027.13745\n",
      "Epoch 241/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 712.8853 - val_loss: 2061.8420\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 2027.13745\n",
      "Epoch 242/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 708.2717 - val_loss: 2128.1213\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 2027.13745\n",
      "Epoch 243/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 738.1208 - val_loss: 2113.9917\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 2027.13745\n",
      "Epoch 244/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 698.0357 - val_loss: 2115.2180\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 2027.13745\n",
      "Epoch 245/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 733.0027 - val_loss: 2099.9172\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 2027.13745\n",
      "Epoch 246/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 735.6729 - val_loss: 2117.6260\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 2027.13745\n",
      "Epoch 247/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 725.1158 - val_loss: 2105.6736\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 2027.13745\n",
      "Epoch 248/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 740.9290 - val_loss: 2117.2219\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 2027.13745\n",
      "Epoch 249/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 715.7271 - val_loss: 2079.0950\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 2027.13745\n",
      "Epoch 250/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 737.7174 - val_loss: 2100.6072\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 2027.13745\n",
      "Epoch 251/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 727.8574 - val_loss: 2145.8987\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 2027.13745\n",
      "Epoch 252/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 715.1576 - val_loss: 2051.8352\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 2027.13745\n",
      "Epoch 253/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 742.3793 - val_loss: 2119.1399\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 2027.13745\n",
      "Epoch 254/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 713.1586 - val_loss: 2108.7378\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 2027.13745\n",
      "Epoch 255/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 702.3082 - val_loss: 2136.1951\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 2027.13745\n",
      "Epoch 256/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 736.0726 - val_loss: 2095.3044\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 2027.13745\n",
      "Epoch 257/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 706.4510 - val_loss: 2063.8840\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 2027.13745\n",
      "Epoch 258/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 717.4382 - val_loss: 2070.2188\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 2027.13745\n",
      "Epoch 259/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 708.7730 - val_loss: 2101.0007\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 2027.13745\n",
      "Epoch 260/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 713.4237 - val_loss: 2125.8516\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 2027.13745\n",
      "Epoch 261/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 726.1336 - val_loss: 2069.5642\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 2027.13745\n",
      "Epoch 262/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 711.8258 - val_loss: 2156.5911\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 2027.13745\n",
      "Epoch 263/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 719.3083 - val_loss: 2068.0408\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 2027.13745\n",
      "Epoch 264/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 711.0173 - val_loss: 2077.4490\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 2027.13745\n",
      "Epoch 265/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 699.5132 - val_loss: 2069.2981\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 2027.13745\n",
      "Epoch 266/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 720.8899 - val_loss: 2138.0483\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 2027.13745\n",
      "Epoch 267/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 705.2045 - val_loss: 2081.7209\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 2027.13745\n",
      "Epoch 268/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 718.0301 - val_loss: 2076.7388\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 2027.13745\n",
      "Epoch 269/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 718.2093 - val_loss: 2087.8630\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 2027.13745\n",
      "Epoch 270/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 714.8427 - val_loss: 2110.7896\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 2027.13745\n",
      "Epoch 271/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 727.6786 - val_loss: 2035.8162\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 2027.13745\n",
      "Epoch 272/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 701.8779 - val_loss: 2103.4783\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 2027.13745\n",
      "Epoch 273/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 685.1146 - val_loss: 2162.7849\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 2027.13745\n",
      "Epoch 274/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 717.0419 - val_loss: 2088.9309\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 2027.13745\n",
      "Epoch 275/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 690.0920 - val_loss: 2087.6826\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 2027.13745\n",
      "Epoch 276/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 719.6805 - val_loss: 2087.2195\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 2027.13745\n",
      "Epoch 277/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 717.4677 - val_loss: 2085.3877\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 2027.13745\n",
      "Epoch 278/500\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 708.1556 - val_loss: 2063.6235\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 2027.13745\n",
      "Epoch 279/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 696.9332 - val_loss: 2081.6428\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 2027.13745\n",
      "Epoch 280/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 704.6336 - val_loss: 2050.3154\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 2027.13745\n",
      "Epoch 281/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 709.7607 - val_loss: 2085.6467\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 2027.13745\n",
      "Epoch 282/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 682.3041 - val_loss: 2090.7854\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 2027.13745\n",
      "Epoch 283/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 689.1818 - val_loss: 2176.1389\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 2027.13745\n",
      "Epoch 284/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 709.7029 - val_loss: 2050.0337\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 2027.13745\n",
      "Epoch 285/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 706.6489 - val_loss: 2116.0425\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 2027.13745\n",
      "Epoch 286/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 709.4764 - val_loss: 2087.3013\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 2027.13745\n",
      "Epoch 287/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 703.4968 - val_loss: 2081.8635\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 2027.13745\n",
      "Epoch 288/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 722.6270 - val_loss: 2079.5588\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 2027.13745\n",
      "Epoch 289/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 710.2399 - val_loss: 2078.1072\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 2027.13745\n",
      "Epoch 290/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 728.2346 - val_loss: 2078.5527\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 2027.13745\n",
      "Epoch 291/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 689.8203 - val_loss: 2042.6831\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 2027.13745\n",
      "Epoch 292/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 684.3217 - val_loss: 2044.5616\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 2027.13745\n",
      "Epoch 293/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 689.7388 - val_loss: 2045.1025\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 2027.13745\n",
      "Epoch 294/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 710.5575 - val_loss: 2070.9998\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 2027.13745\n",
      "Epoch 295/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 699.7591 - val_loss: 2085.0991\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 2027.13745\n",
      "Epoch 296/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 691.7070 - val_loss: 2070.4802\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 2027.13745\n",
      "Epoch 297/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 706.5148 - val_loss: 2084.5906\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 2027.13745\n",
      "Epoch 298/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 707.1554 - val_loss: 2076.3630\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 2027.13745\n",
      "Epoch 299/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 682.7805 - val_loss: 2049.4783\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 2027.13745\n",
      "Epoch 300/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 703.8856 - val_loss: 2030.3580\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 2027.13745\n",
      "Epoch 301/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 718.4814 - val_loss: 2062.0076\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 2027.13745\n",
      "Epoch 302/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 687.1354 - val_loss: 2067.4436\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 2027.13745\n",
      "Epoch 303/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 654.5483 - val_loss: 2097.0769\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 2027.13745\n",
      "Epoch 304/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 703.2496 - val_loss: 2110.9983\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 2027.13745\n",
      "Epoch 305/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 713.5970 - val_loss: 2078.9990\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 2027.13745\n",
      "Epoch 306/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 678.9130 - val_loss: 2161.8528\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 2027.13745\n",
      "Epoch 307/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 693.1536 - val_loss: 2074.7957\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 2027.13745\n",
      "Epoch 308/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 705.5820 - val_loss: 2091.4690\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 2027.13745\n",
      "Epoch 309/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 691.5668 - val_loss: 2074.4622\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 2027.13745\n",
      "Epoch 310/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 667.0649 - val_loss: 2114.5198\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 2027.13745\n",
      "Epoch 311/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 702.1605 - val_loss: 2131.4597\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 2027.13745\n",
      "Epoch 312/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 669.2996 - val_loss: 2077.8801\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 2027.13745\n",
      "Epoch 313/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 709.1009 - val_loss: 2109.6951\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 2027.13745\n",
      "Epoch 314/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 711.0364 - val_loss: 2108.2612\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 2027.13745\n",
      "Epoch 315/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 705.0311 - val_loss: 2094.1543\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 2027.13745\n",
      "Epoch 316/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 682.6417 - val_loss: 2062.7075\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 2027.13745\n",
      "Epoch 317/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 694.1740 - val_loss: 2057.7627\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 2027.13745\n",
      "Epoch 318/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 687.7336 - val_loss: 2072.6348\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 2027.13745\n",
      "Epoch 319/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 687.4359 - val_loss: 2105.0955\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 2027.13745\n",
      "Epoch 320/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 684.6854 - val_loss: 2076.4160\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 2027.13745\n",
      "Epoch 321/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 693.9447 - val_loss: 2060.0176\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 2027.13745\n",
      "Epoch 322/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 684.1069 - val_loss: 2022.7808\n",
      "\n",
      "Epoch 00322: val_loss improved from 2027.13745 to 2022.78076, saving model to model_params.h5\n",
      "Epoch 323/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 678.2864 - val_loss: 2091.0974\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 2022.78076\n",
      "Epoch 324/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 689.6672 - val_loss: 2068.0962\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 2022.78076\n",
      "Epoch 325/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 686.5080 - val_loss: 2030.8882\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 2022.78076\n",
      "Epoch 326/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 675.0703 - val_loss: 2054.4634\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 2022.78076\n",
      "Epoch 327/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 694.8578 - val_loss: 2103.1294\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 2022.78076\n",
      "Epoch 328/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 686.4836 - val_loss: 2046.8475\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 2022.78076\n",
      "Epoch 329/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 699.9923 - val_loss: 2062.6877\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 2022.78076\n",
      "Epoch 330/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 675.5758 - val_loss: 2077.0615\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 2022.78076\n",
      "Epoch 331/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 683.3222 - val_loss: 2077.7017\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 2022.78076\n",
      "Epoch 332/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 680.9138 - val_loss: 2022.7438\n",
      "\n",
      "Epoch 00332: val_loss improved from 2022.78076 to 2022.74377, saving model to model_params.h5\n",
      "Epoch 333/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 696.9326 - val_loss: 2049.2561\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 2022.74377\n",
      "Epoch 334/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 676.2492 - val_loss: 2074.3833\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 2022.74377\n",
      "Epoch 335/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 699.4550 - val_loss: 2043.7466\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 2022.74377\n",
      "Epoch 336/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 674.1240 - val_loss: 2047.3044\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 2022.74377\n",
      "Epoch 337/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 690.2177 - val_loss: 2062.5627\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 2022.74377\n",
      "Epoch 338/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 679.0035 - val_loss: 2049.3010\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 2022.74377\n",
      "Epoch 339/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 652.6448 - val_loss: 2047.4238\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 2022.74377\n",
      "Epoch 340/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 688.9249 - val_loss: 2048.1680\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 2022.74377\n",
      "Epoch 341/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 696.3796 - val_loss: 2041.1113\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 2022.74377\n",
      "Epoch 342/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 684.0050 - val_loss: 2090.7859\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 2022.74377\n",
      "Epoch 343/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 663.7692 - val_loss: 2102.9949\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 2022.74377\n",
      "Epoch 344/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 678.3139 - val_loss: 2126.4458\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 2022.74377\n",
      "Epoch 345/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 643.7525 - val_loss: 2095.4124\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 2022.74377\n",
      "Epoch 346/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 688.8434 - val_loss: 2061.4036\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 2022.74377\n",
      "Epoch 347/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 675.5050 - val_loss: 2061.4626\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 2022.74377\n",
      "Epoch 348/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 703.5089 - val_loss: 2054.9038\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 2022.74377\n",
      "Epoch 349/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 666.4755 - val_loss: 2058.9888\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 2022.74377\n",
      "Epoch 350/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 686.0172 - val_loss: 2115.0386\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 2022.74377\n",
      "Epoch 351/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 678.2381 - val_loss: 2095.9326\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 2022.74377\n",
      "Epoch 352/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 679.1906 - val_loss: 2090.9556\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 2022.74377\n",
      "Epoch 353/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 711.6207 - val_loss: 2084.2004\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 2022.74377\n",
      "Epoch 354/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 703.4531 - val_loss: 2117.4573\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 2022.74377\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 647.7775 - val_loss: 2126.7793\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 2022.74377\n",
      "Epoch 356/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 674.8207 - val_loss: 2146.3047\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 2022.74377\n",
      "Epoch 357/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 695.4199 - val_loss: 2078.5608\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 2022.74377\n",
      "Epoch 358/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 672.2978 - val_loss: 2038.2349\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 2022.74377\n",
      "Epoch 359/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 700.5427 - val_loss: 2078.8577\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 2022.74377\n",
      "Epoch 360/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 670.0131 - val_loss: 2118.3289\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 2022.74377\n",
      "Epoch 361/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 704.8164 - val_loss: 2094.0198\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 2022.74377\n",
      "Epoch 362/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 679.0143 - val_loss: 2045.4512\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 2022.74377\n",
      "Epoch 363/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 685.3773 - val_loss: 2067.2627\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 2022.74377\n",
      "Epoch 364/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 673.2623 - val_loss: 2044.7703\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 2022.74377\n",
      "Epoch 365/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 694.9450 - val_loss: 2095.7466\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 2022.74377\n",
      "Epoch 366/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 692.5511 - val_loss: 2083.6509\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 2022.74377\n",
      "Epoch 367/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 665.9081 - val_loss: 2098.0869\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 2022.74377\n",
      "Epoch 368/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 687.6902 - val_loss: 2060.9062\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 2022.74377\n",
      "Epoch 369/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 679.6621 - val_loss: 2111.1965\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 2022.74377\n",
      "Epoch 370/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 692.2950 - val_loss: 2064.1658\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 2022.74377\n",
      "Epoch 371/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 680.6376 - val_loss: 2069.7310\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 2022.74377\n",
      "Epoch 372/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 670.8258 - val_loss: 2069.2090\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 2022.74377\n",
      "Epoch 373/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 669.2878 - val_loss: 2059.0959\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 2022.74377\n",
      "Epoch 374/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 683.1935 - val_loss: 2100.3398\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 2022.74377\n",
      "Epoch 375/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 690.4023 - val_loss: 2081.7454\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 2022.74377\n",
      "Epoch 376/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 678.2079 - val_loss: 2146.4268\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 2022.74377\n",
      "Epoch 377/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 699.4315 - val_loss: 2085.3950\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 2022.74377\n",
      "Epoch 378/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 664.5964 - val_loss: 2066.4385\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 2022.74377\n",
      "Epoch 379/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 679.2432 - val_loss: 2106.7244\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 2022.74377\n",
      "Epoch 380/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 693.7441 - val_loss: 2069.8259\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 2022.74377\n",
      "Epoch 381/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 674.5610 - val_loss: 2081.8289\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 2022.74377\n",
      "Epoch 382/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 663.1474 - val_loss: 2055.5815\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 2022.74377\n",
      "Epoch 383/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 635.0543 - val_loss: 2033.9191\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 2022.74377\n",
      "Epoch 384/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 664.6012 - val_loss: 2073.1199\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 2022.74377\n",
      "Epoch 385/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 668.8883 - val_loss: 2107.9248\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 2022.74377\n",
      "Epoch 386/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 663.4141 - val_loss: 2080.0439\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 2022.74377\n",
      "Epoch 387/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 646.4382 - val_loss: 2151.3503\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 2022.74377\n",
      "Epoch 388/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 680.7783 - val_loss: 2057.2336\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 2022.74377\n",
      "Epoch 389/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 693.2216 - val_loss: 2074.5500\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 2022.74377\n",
      "Epoch 390/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 671.7670 - val_loss: 2070.7234\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 2022.74377\n",
      "Epoch 391/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 675.3373 - val_loss: 2049.5359\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 2022.74377\n",
      "Epoch 392/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 648.9152 - val_loss: 2055.4519\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 2022.74377\n",
      "Epoch 393/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 663.0158 - val_loss: 2108.4016\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 2022.74377\n",
      "Epoch 394/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 660.5455 - val_loss: 2081.6479\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 2022.74377\n",
      "Epoch 395/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 686.1308 - val_loss: 2078.8752\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 2022.74377\n",
      "Epoch 396/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 702.6996 - val_loss: 2082.2722\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 2022.74377\n",
      "Epoch 397/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 659.4844 - val_loss: 2129.9353\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 2022.74377\n",
      "Epoch 398/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 669.6041 - val_loss: 2084.4072\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 2022.74377\n",
      "Epoch 399/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 684.9032 - val_loss: 2080.4065\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 2022.74377\n",
      "Epoch 400/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 662.1207 - val_loss: 2067.8750\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 2022.74377\n",
      "Epoch 401/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 670.3645 - val_loss: 2102.4233\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 2022.74377\n",
      "Epoch 402/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 685.8273 - val_loss: 2071.2471\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 2022.74377\n",
      "Epoch 403/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 685.2988 - val_loss: 2109.3591\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 2022.74377\n",
      "Epoch 404/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 656.7978 - val_loss: 2098.0835\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 2022.74377\n",
      "Epoch 405/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 650.8226 - val_loss: 2078.0723\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 2022.74377\n",
      "Epoch 406/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 654.3290 - val_loss: 2110.6257\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 2022.74377\n",
      "Epoch 407/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 681.0138 - val_loss: 2119.3730\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 2022.74377\n",
      "Epoch 408/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 665.3157 - val_loss: 2088.6069\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 2022.74377\n",
      "Epoch 409/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 662.1252 - val_loss: 2065.2205\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 2022.74377\n",
      "Epoch 410/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 680.3138 - val_loss: 2077.9158\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 2022.74377\n",
      "Epoch 411/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 677.5853 - val_loss: 2071.6326\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 2022.74377\n",
      "Epoch 412/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 699.5453 - val_loss: 2046.5519\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 2022.74377\n",
      "Epoch 413/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 649.3631 - val_loss: 2058.3013\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 2022.74377\n",
      "Epoch 414/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 659.6248 - val_loss: 2066.0493\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 2022.74377\n",
      "Epoch 415/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 680.8804 - val_loss: 2090.4263\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 2022.74377\n",
      "Epoch 416/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 682.7464 - val_loss: 2110.2009\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 2022.74377\n",
      "Epoch 417/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 667.5314 - val_loss: 2078.5186\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 2022.74377\n",
      "Epoch 418/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 665.9332 - val_loss: 2051.4578\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 2022.74377\n",
      "Epoch 419/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 673.5012 - val_loss: 2035.2247\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 2022.74377\n",
      "Epoch 420/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 669.5613 - val_loss: 2078.7642\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 2022.74377\n",
      "Epoch 421/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 658.5193 - val_loss: 2041.4907\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 2022.74377\n",
      "Epoch 422/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 648.9188 - val_loss: 2077.1921\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 2022.74377\n",
      "Epoch 423/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 669.9412 - val_loss: 2034.2571\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 2022.74377\n",
      "Epoch 424/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 656.3897 - val_loss: 2065.5754\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 2022.74377\n",
      "Epoch 425/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 674.4080 - val_loss: 2071.4580\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 2022.74377\n",
      "Epoch 426/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 655.4202 - val_loss: 2101.7488\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 2022.74377\n",
      "Epoch 427/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 669.0483 - val_loss: 2040.0334\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 2022.74377\n",
      "Epoch 428/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 658.8722 - val_loss: 2066.5811\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 2022.74377\n",
      "Epoch 429/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 684.0555 - val_loss: 2084.7786\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 2022.74377\n",
      "Epoch 430/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 673.5349 - val_loss: 2182.8730\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 2022.74377\n",
      "Epoch 431/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 637.9551 - val_loss: 2114.8455\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 2022.74377\n",
      "Epoch 432/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 666.3017 - val_loss: 2113.3892\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 2022.74377\n",
      "Epoch 433/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 670.9552 - val_loss: 2096.5679\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 2022.74377\n",
      "Epoch 434/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 684.5933 - val_loss: 2077.0781\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 2022.74377\n",
      "Epoch 435/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 657.6096 - val_loss: 2085.0310\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 2022.74377\n",
      "Epoch 436/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 651.2927 - val_loss: 2049.9458\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 2022.74377\n",
      "Epoch 437/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 650.0590 - val_loss: 2068.4768\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 2022.74377\n",
      "Epoch 438/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 650.0291 - val_loss: 2106.1680\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 2022.74377\n",
      "Epoch 439/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 661.9717 - val_loss: 2077.1899\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 2022.74377\n",
      "Epoch 440/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 665.9001 - val_loss: 2061.3381\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 2022.74377\n",
      "Epoch 441/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 634.3077 - val_loss: 2096.3801\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 2022.74377\n",
      "Epoch 442/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 661.9117 - val_loss: 2084.9939\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 2022.74377\n",
      "Epoch 443/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 675.7133 - val_loss: 2101.3889\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 2022.74377\n",
      "Epoch 444/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 686.3397 - val_loss: 2093.0334\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 2022.74377\n",
      "Epoch 445/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 659.2424 - val_loss: 2102.8049\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 2022.74377\n",
      "Epoch 446/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 664.0710 - val_loss: 2096.6680\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 2022.74377\n",
      "Epoch 447/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 650.8079 - val_loss: 2072.3792\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 2022.74377\n",
      "Epoch 448/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 657.3384 - val_loss: 2069.4414\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 2022.74377\n",
      "Epoch 449/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 659.7038 - val_loss: 2136.5222\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 2022.74377\n",
      "Epoch 450/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 651.3843 - val_loss: 2124.8115\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 2022.74377\n",
      "Epoch 451/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 678.8573 - val_loss: 2070.7979\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 2022.74377\n",
      "Epoch 452/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 680.9101 - val_loss: 2114.6443\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 2022.74377\n",
      "Epoch 453/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 650.7570 - val_loss: 2084.2971\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 2022.74377\n",
      "Epoch 454/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 672.7429 - val_loss: 2082.1650\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 2022.74377\n",
      "Epoch 455/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 679.5356 - val_loss: 2145.7097\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 2022.74377\n",
      "Epoch 456/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 667.0980 - val_loss: 2094.6531\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 2022.74377\n",
      "Epoch 457/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 658.1564 - val_loss: 2131.1123\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 2022.74377\n",
      "Epoch 458/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 652.0242 - val_loss: 2111.3110\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 2022.74377\n",
      "Epoch 459/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 659.4519 - val_loss: 2133.4929\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 2022.74377\n",
      "Epoch 460/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 654.3264 - val_loss: 2141.7891\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 2022.74377\n",
      "Epoch 461/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 677.6243 - val_loss: 2145.3276\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 2022.74377\n",
      "Epoch 462/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 669.0126 - val_loss: 2099.8044\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 2022.74377\n",
      "Epoch 463/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 651.3673 - val_loss: 2100.4702\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 2022.74377\n",
      "Epoch 464/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 649.4979 - val_loss: 2078.4885\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 2022.74377\n",
      "Epoch 465/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 682.5092 - val_loss: 2229.0608\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 2022.74377\n",
      "Epoch 466/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 674.1869 - val_loss: 2130.4761\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 2022.74377\n",
      "Epoch 467/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 648.9848 - val_loss: 2161.8804\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 2022.74377\n",
      "Epoch 468/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 660.1055 - val_loss: 2093.3413\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 2022.74377\n",
      "Epoch 469/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 629.6631 - val_loss: 2088.7300\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 2022.74377\n",
      "Epoch 470/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 656.6467 - val_loss: 2092.1560\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 2022.74377\n",
      "Epoch 471/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 661.8768 - val_loss: 2074.1265\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 2022.74377\n",
      "Epoch 472/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 635.4161 - val_loss: 2072.5479\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 2022.74377\n",
      "Epoch 473/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 663.9646 - val_loss: 2117.5513\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 2022.74377\n",
      "Epoch 474/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 638.8611 - val_loss: 2080.1597\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 2022.74377\n",
      "Epoch 475/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 657.8274 - val_loss: 2131.7351\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 2022.74377\n",
      "Epoch 476/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 643.0942 - val_loss: 2112.1465\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 2022.74377\n",
      "Epoch 477/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 644.7527 - val_loss: 2120.4246\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 2022.74377\n",
      "Epoch 478/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 671.3751 - val_loss: 2112.1885\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 2022.74377\n",
      "Epoch 479/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 666.8453 - val_loss: 2092.8718\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 2022.74377\n",
      "Epoch 480/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 675.7659 - val_loss: 2218.5579\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 2022.74377\n",
      "Epoch 481/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 633.4261 - val_loss: 2090.7461\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 2022.74377\n",
      "Epoch 482/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 668.9404 - val_loss: 2103.7815\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 2022.74377\n",
      "Epoch 483/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 632.7672 - val_loss: 2108.4739\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 2022.74377\n",
      "Epoch 484/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 668.1024 - val_loss: 2104.5349\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 2022.74377\n",
      "Epoch 485/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 654.2664 - val_loss: 2091.7698\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 2022.74377\n",
      "Epoch 486/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 649.5040 - val_loss: 2095.0410\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 2022.74377\n",
      "Epoch 487/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 645.3066 - val_loss: 2077.0757\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 2022.74377\n",
      "Epoch 488/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 645.7589 - val_loss: 2069.0098\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 2022.74377\n",
      "Epoch 489/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 629.9775 - val_loss: 2092.6260\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 2022.74377\n",
      "Epoch 490/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 653.0714 - val_loss: 2074.3149\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 2022.74377\n",
      "Epoch 491/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 671.0695 - val_loss: 2065.3936\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 2022.74377\n",
      "Epoch 492/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 627.3746 - val_loss: 2071.3240\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 2022.74377\n",
      "Epoch 493/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 631.7136 - val_loss: 2112.6760\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 2022.74377\n",
      "Epoch 494/500\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 687.0209 - val_loss: 2086.8560\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 2022.74377\n",
      "Epoch 495/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 649.4887 - val_loss: 2080.7249\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 2022.74377\n",
      "Epoch 496/500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 659.9532 - val_loss: 2084.3579\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 2022.74377\n",
      "Epoch 497/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 661.2835 - val_loss: 2052.8994\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 2022.74377\n",
      "Epoch 498/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 669.6698 - val_loss: 2093.1792\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 2022.74377\n",
      "Epoch 499/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 657.9390 - val_loss: 2070.0320\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 2022.74377\n",
      "Epoch 500/500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 637.2193 - val_loss: 2111.2573\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 2022.74377\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Dropout, GlobalMaxPooling1D, Input\n",
    "from keras.models import Model\n",
    "\n",
    "ip = Input(shape=(max_tokens, dim), name='ip')\n",
    "conv1 = Conv1D(128, 1, activation='tanh')(ip)\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "drop = Dropout(0.5)(pool1)\n",
    "output = Dense(3)(drop)\n",
    "\n",
    "model = Model(inputs = ip, outputs=output)\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = \"sgd\")\n",
    "checkpoint = ModelCheckpoint(\"model_params.h5\", monitor = \"val_loss\", verbose = 1, save_best_only = True)\n",
    "model.fit(X_train, Y_train, epochs = 500, validation_split = 0.1, callbacks = [checkpoint])\n",
    "model.load_weights(\"model_params.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def display_colour(rgb):\n",
    "    rgb = rgb.astype('float')\n",
    "\n",
    "    rgb /= 255\n",
    "    (h, w) = (64, 64)\n",
    "    colour_swatch = np.zeros((h, w, 3))\n",
    "    for row in range(h):\n",
    "        for col in range(w):\n",
    "            colour_swatch[row, col] = rgb\n",
    "\n",
    "    plt.imshow(colour_swatch)\n",
    "    plt.show()\n",
    "    \n",
    "def generate_colour(name):\n",
    "\n",
    "    words = name.lower().split()\n",
    "    X_test = np.zeros((max_tokens, dim))\n",
    "    for (i, word) in enumerate(words):\n",
    "        X_test[i] = word2vec[word]\n",
    "\n",
    "    rgb = model.predict(np.array([X_test]))[0]\n",
    "    print(type(rgb))\n",
    "    display_colour(rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_colour(\"Green\")\n",
    "generate_colour(\"Booger Green\")\n",
    "generate_colour(\"Rainforest Green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_colour(\"Purple\")\n",
    "generate_colour(\"Deep Purple\")\n",
    "generate_colour(\"Violet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_colour(\"Blue\")\n",
    "generate_colour(\"Deep Blue\")\n",
    "generate_colour(\"Deep Ocean Blue\")\n",
    "generate_colour(\"Light Blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
